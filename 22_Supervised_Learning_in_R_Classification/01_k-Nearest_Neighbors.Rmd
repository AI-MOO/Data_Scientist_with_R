---
title: "k-Nearest Neighbors (kNN)"
author: "Mohamad Osman"
date: '2022-07-25'
output: rmarkdown::github_document
---

# Section 01: k-Nearest Neighbors (kNN)

### **`01-Recognizing a road sign with kNN`**

After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.

As it begins to drive away, its camera captures the following image:

![Stop Sign](https://assets.datacamp.com/production/course_2906/datasets/knn_stop_99.gif){alt="Stop Sign"}

Can you apply a kNN classifier to help the car recognize this sign?

The dataset `signs` is loaded in your workspace along with the data frame `next_sign`, which holds the observation you want to classify.

```{r}
library(dplyr)
library(readr)
library(ggplot2)

URL <- "https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv"

signs <- read_csv(URL)

test_signs <- signs %>%
         filter(sample == "test")
drops <- c("id", "sample")
test_signs <- test_signs[ ,!(names(test_signs) %in% drops)]
test_signs


signs <- signs %>%
         filter(sample == "train")
drops <- c("id", "sample")
signs <- signs[ ,!(names(signs) %in% drops)]
signs


values <- c(204, 227, 220, 196, 59, 51, 202, 67, 59, 204, 227, 220, 236, 250, 234, 242, 252, 235, 205, 148, 131, 190, 50, 43, 179, 70, 57, 242, 229, 212, 190, 50,  43, 193,  51,  44, 170, 197, 196, 190, 50,  43, 190,  47, 41, 165, 195, 196)

cols_names <- c("r1","g1","b1","r2","g2","b2","r3","g3","b3","r4","g4","b4","r5","g5","b5","r6","g6","b6","r7","g7","b7","r8","g8","b8","r9","g9","b9","r10","g10","b10","r11","g11","b11","r12","g12","b12","r13","g13","b13","r14","g14","b14","r15","g15","b15","r16","g16","b16")


next_sign <- data.frame(matrix(values, ncol = 48, nrow = 1))
colnames(next_sign) <- cols_names
next_sign
```

-   Load the `class` package.

-   Create a vector of sign labels to use with kNN by extracting the column `sign_type` from `signs`.

-   Identify the `next_sign` using the `knn()` function.

    -   Set the `train` argument equal to the `signs` data frame *without* the first column.

    -   Set the `test` argument equal to the data frame `next_sign`.

    -   Use the vector of labels you created as the `cl` argument.

```{r}
# Load the 'class' package
library(class)

# Create a vector of labels
sign_types <- signs$sign_type

# Classify the next sign observed
knn(train = signs[-1], test = next_sign, cl = sign_types)
```

### **`02-Thinking like kNN`**

With your help, the test car successfully identified the sign and stopped safely at the intersection.

How did the `knn()` function correctly classify the stop sign?

`Ans` : The sign was in some way similar to another stop sign

### 
**`03-Exploring the traffic sign dataset`**

To better understand how the `knn()` function was able to classify the stop sign, it may help to examine the training dataset it used.

Each previously observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here.

![Stop Sign Data Encoding](https://assets.datacamp.com/production/course_2906/datasets/knn_sign_data.png){alt="Stop Sign Data Encoding"}

The result is a dataset that records the `sign_type` as well as 16 x 3 = 48 color properties of each sign.

-   Use the `str()` function to examine the `signs` dataset.

-   Use `table()` to count the number of observations of each sign type by passing it the column containing the labels.

-   Run the provided `aggregate()` command to see whether the average red level might vary by sign type.

```{r}
# Examine the structure of the signs dataset
str(signs)

# Count the number of signs of each type
table(signs$sign_type)

# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = signs, mean)
```

### **`04-Classifying a collection of road signs`**

Now that the autonomous vehicle has successfully stopped on its own, your team feels confident allowing the car to continue the test course.

The test course includes 59 additional road signs divided into three types:

![Stop Sign](https://assets.datacamp.com/production/course_2906/datasets/knn_stop_28.gif){alt="Stop Sign"} ![Speed Limit Sign](https://assets.datacamp.com/production/course_2906/datasets/knn_speed_55.gif){alt="Speed Limit Sign"} ![Pedestrian Sign](https://assets.datacamp.com/production/course_2906/datasets/knn_peds_47.gif){alt="Pedestrian Sign"}

At the conclusion of the trial, you are asked to measure the car's overall performance at recognizing these signs.

The `class` package and the dataset `signs` are already loaded in your workspace. So is the data frame `test_signs`, which holds a set of observations you'll test your model on.

-   Classify the `test_signs` data using `knn()`.

    -   Set `train` equal to the observations in `signs` *without* labels.

    -   Use `test_signs` for the `test` argument, again without labels.

    -   For the `cl` argument, use the vector of labels provided for you.

-   Use `table()` to explore the classifier's performance at identifying the three sign types (the confusion matrix).

    -   Create the vector `signs_actual` by extracting the labels from `test_signs`.

    -   Pass the vector of predictions and the vector of actual signs to `table()` to cross tabulate them.

-   Compute the overall accuracy of the kNN learner using the `mean()` function.

```{r}
# Use kNN to identify the test road signs
sign_types <- signs$sign_type
signs_pred <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types)

# Create a confusion matrix of the predicted versus actual values
signs_actual <- test_signs$sign_type
table(signs_pred, signs_actual)

# Compute the accuracy
mean(signs_pred == signs_actual)
```

### `05-Understanding the impact of 'k'`

There is a complex relationship between k and classification accuracy. Bigger is not always better.

Which of these is a valid reason for keeping k as small as possible (but no smaller)?

`Answer:` A smaller k may utilize more subtle patterns

### 
`06-Testing other 'k' values`

By default, the `knn()` function in the `class` package uses only the single nearest neighbor.

Setting a `k` parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class.

Compare `k` values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.

The `class` package is already loaded in your workspace along with the datasets `signs`, `signs_test`, and `sign_types`. The object `signs_actual` holds the true values of the signs.

-   Compute the accuracy of the default `k = 1` model using the given code, then find the accuracy of the model using `mean()` to compare `signs_actual` and the model's predictions.

-   Modify the `knn()` function call by setting `k = 7` and again find accuracy value.

-   Revise the code once more by setting `k = 15`, plus find the accuracy value one more time.

```{r}
signs_test <- test_signs

# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types)
mean(k_1 == signs_actual)

# Modify the above to set k = 7
k_7 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 7)
mean(k_7 == signs_actual)

# Set k = 15 and compare to the above
k_15 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 15)
mean(k_15 == signs_actual)
```

### **`07-Seeing how the neighbors voted`**

When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated.

For example, knowing more about the voters' confidence in the classification could allow an autonomous vehicle to use caution in the case there is *any chance at all* that a stop sign is ahead.

In this exercise, you will learn how to obtain the voting results from the `knn()` function.

The `class` package has already been loaded in your workspace along with the datasets `signs`, `sign_types`, and `signs_test`.

-   Build a kNN model with the `prob = TRUE` parameter to compute the vote proportions. Set `k = 7`.

-   Use the `attr()` function to obtain the vote proportions for the predicted class. These are stored in the attribute `"prob"`.

-   Examine the first several vote outcomes and percentages using the `head()` function to see how the confidence varies from sign to sign.

```{r}
# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <-  knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 7, prob = TRUE)

# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")

# Examine the first several predictions
print("------------------------")
head(sign_pred)

# Examine the proportion of votes for the winning class
print("------------------------")
head(sign_prob)
```

### **`08-Why normalize data?`**

![](images/chrome_iIA0eXwryy.png){width="454"}

Before applying kNN to a classification task, it is common practice to rescale the data using a technique like **min-max normalization**. What is the purpose of this step?

`Answer:` To ensure all data elements may contribute equal shares to distance.

Yes! Rescaling reduces the influence of extreme values on kNN's distance function.

### **`The End`** 






 




