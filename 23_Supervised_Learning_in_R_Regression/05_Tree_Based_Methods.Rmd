---
title: "Tree-Based Methods"
author: "Mohamad Osman"
date: '2022-08-03'
output: rmarkdown::github_document
---

# Section 05: **Tree-Based Methods**

```{r}
library(dplyr)
library(ggplot2)
library(tidyr)
library(magrittr)

file <- file.path("..", "00_Datasets", "Bikes.RData")
(load(file))
```

### 
**`01-Predicting with a decision tree`**

![decision tree](https://assets.datacamp.com/production/course_3851/datasets/decisiontree_intelligence.png){alt="decision tree" width="337"}

Here you see the decision tree learned from the brain dataset shown in the previous video. The tree predicts the expected intelligence (humans have an intelligence of 1) of several mammals, as a function of *gestation time* (in days) and average *litter size* for that species.

The leaf nodes show the expected brain size for the datums in that node, as well as how many (percentage-wise) of the datums fall into the node.

You want to predict the intelligence of a gorilla, which has a gestation time of 265 days and an average litter size of 1.

What relative brain size does this tree model predict?

-   0.073

-   0.131

-   0.148 ✅

-   0.161

-   0.274

-   0.315

Yes! But as you see this tree can predict only 6 values. For the rest of this chapter we will use tree ensembles to predict at a finer granularity.

### **`02-Build a random forest model for bike rentals`**

In this exercise, you will again build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. You will train the model on data from the month of July.

You will use the `ranger` package to fit the random forest model. For this exercise, the key arguments to the [**`ranger()`**](https://www.rdocumentation.org/packages/ranger/topics/ranger) call are:

-   `formula`

-   `data`

-   `num.trees`: the number of trees in the forest.

-   `respect.unordered.factors` : Specifies how to treat unordered factor variables. We recommend setting this to "order" for regression.

-   `seed`: because this is a random algorithm, you will set the seed to get reproducible results

Since there are a lot of input variables, for convenience we will specify the outcome and the inputs in the variables `outcome` and `vars`, and use [**`paste()`**](https://www.rdocumentation.org/packages/base/topics/paste) to assemble a string representing the model formula.

The data frame `bikesJuly` has been pre-loaded. The sample code specifies the names of the outcome and input variables.

-   Fill in the blanks to create the formula `fmla` expressing `cnt` as a function of the inputs. Print it.

-   Load the package `ranger`.

-   Use `ranger` to fit a model to the `bikesJuly` data: `bike_model_rf`.

    -   The first argument to `ranger()` is the formula, `fmla`.

    -   Use 500 trees and `respect.unordered.factors = "order"`.

    -   Set the seed to `seed` for reproducible results.

    -   Print the model. What is the R-squared?

```{r}
# bikesJuly is available
str(bikesJuly)

# Random seed to reproduce results
seed <- 423563

# The outcome column
(outcome <- "cnt")

# The input variables
(vars <- c("hr", "holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed"))

# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + ")))

# Load the package ranger
library(ranger)

# Fit and print the random forest model
(bike_model_rf <- ranger(fmla, # formula 
                         bikesJuly, # data
                         num.trees = 500, 
                         respect.unordered.factors = "order", 
                         seed = seed))
```

R squared (OOB): `0.8205434`✅

### **`03-Predict bike rentals with the random forest model`**

In this exercise, you will use the model that you fit in the previous exercise to predict bike rentals for the month of August.

The [**`predict()`**](https://www.rdocumentation.org/packages/ranger/topics/predict.ranger) function for a `ranger` model produces a list. One of the elements of this list is `predictions`, a vector of predicted values. You can access `predictions` with the `$` notation for accessing named elements of a list:

`predict(model, data)$predictions`

The model `bike_model_rf` and the dataset `bikesAugust` (for evaluation) have been pre-loaded

-   Call [**`predict()`**](https://www.rdocumentation.org/packages/ranger/topics/predict.ranger) on `bikesAugust` to predict the number of bikes rented in August (`cnt`). Add the predictions to `bikesAugust` as the column `pred`.

-   Fill in the blanks to calculate the root mean squared error of the predictions.

    -   The poisson model you built for this data gave an RMSE of about 112.6. How does this model compare?

-   Fill in the blanks to plot actual bike rental counts (`cnt`) versus the predictions (`pred` on x-axis).

```{r}
# bikesAugust is available
str(bikesAugust)

# bike_model_rf is available
bike_model_rf

# Make predictions on the August data
bikesAugust$pred <- predict(bike_model_rf, bikesAugust)$predictions

# Calculate the RMSE of the predictions
bikesAugust %>% 
  mutate(residual = pred - cnt)  %>% # calculate the residual
  summarize(rmse  = sqrt(mean(residual ^ 2)))      # calculate rmse

# Plot actual outcome vs predictions (predictions on x-axis)
ggplot(bikesAugust, aes(x = pred, y = cnt)) + 
  geom_point() + 
  geom_abline()
```

-   RMSE of poisson model \~= 112.6

-   RMSE of Random Forest \~= 97.2

Good job! This random forest model outperforms the poisson count model on the same data; it is discovering more complex non-linear or non-additive relationships in the data.

### **`04-Visualize random forest bike model predictions`**

In the previous exercise, you saw that the random forest bike model did better on the August data than the quasiposson model, in terms of RMSE.

In this exercise, you will visualize the random forest model's August predictions as a function of time. The corresponding plot from the quasipoisson model that you built in a previous exercise is available for you to compare.

Recall that the quasipoisson model mostly identified the pattern of slow and busy hours in the day, but it somewhat underestimated peak demands. You would like to see how the random forest model compares.

The data frame `bikesAugust` (with predictions) has been made available for you. The plot `quasipoisson_plot` of quasipoisson model predictions as a function of time is shown.

-   Fill in the blanks to plot the predictions and actual counts by hour for the first 14 days of August.

    -   [**`gather`**](https://www.rdocumentation.org/packages/tidyr/topics/gather) the `cnt` and `pred` columns into a column called `value`, with a key called `valuetype`.

    -   Plot `value` as a function of `instant` (day).

*How does the random forest model compare?*

```{r}
first_two_weeks <- bikesAugust %>% 
  # Set start to 0, convert unit to days
  mutate(instant = (instant - min(instant)) / 24) %>% 
  # Gather cnt and pred into a column named value with key valuetype
  gather(key = valuetype, value = value, cnt, pred) %>%
  # Filter for rows in the first two
  filter(instant < 14) 

# Plot predictions and cnt by date/time 
ggplot(first_two_weeks, aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
  scale_color_brewer(palette = "Dark2") + 
  ggtitle("Predicted August bike rentals, Random Forest plot")
```

### **`05-vtreat on a small example`**

In this exercise, you will use `vtreat` to one-hot-encode a categorical variable on a small example. `vtreat` creates a *treatment plan* to transform categorical variables into indicator variables (coded `"lev"`), and to clean bad values out of numerical variables (coded `"clean"`).

To design a treatment plan use the function [**`designTreatmentsZ()`**](https://www.rdocumentation.org/packages/vtreat/topics/designTreatmentsZ)

    treatplan <- designTreatmentsZ(data, varlist)

-   `data`: the original training data frame

-   `varlist`: a vector of input variables to be treated (as strings).

`designTreatmentsZ()` returns a list with an element `scoreFrame`: a data frame that includes the names and types of the new variables:

    scoreFrame <- treatplan %>% 
                magrittr::use_series(scoreFrame) %>% 
                select(varName, origName, code)

-   `varName`: the name of the new treated variable

-   `origName`: the name of the original variable that the treated variable comes from

-   `code`: the type of the new variable.

    -   `"clean"`: a numerical variable with no NAs or NaNs

    -   `"lev"`: an indicator variable for a specific level of the original categorical variable.

([**`magrittr::use_series()`**](https://www.rdocumentation.org/packages/magrittr/topics/extract) is an alias for `$` that you can use in pipes.)

For these exercises, we want `varName` where `code` is either `"clean"` or `"lev"`:

    newvarlist <- scoreFrame %>% 
                 filter(code %in% c("clean", "lev") %>%
                 magrittr::use_series(varName)

To transform the dataset into all numerical and one-hot-encoded variables, use [**`prepare()`**](https://www.rdocumentation.org/packages/vtreat/topics/prepare):

    data.treat <- prepare(treatplan, data, varRestrictions = newvarlist)

-   `treatplan`: the treatment plan

-   `data`: the data frame to be treated

-   `varRestrictions`: the variables desired in the treated data

The `dframe` data frame and the `magrittr` package have been pre-loaded.

-   Print `dframe`. We will assume that `color` and `size` are input variables, and `popularity` is the outcome to be predicted.

-   Create a vector called `vars` with the names of the input variables (as strings).

-   Load the package `vtreat`.

-   Use `designTreatmentsZ()` to create a treatment plan for the variables in `vars`. Assign it to the variable `treatplan`.

-   Get and examine the `scoreFrame` from the treatment plan to see the mapping from old variables to new variables.

    -   You only need the columns `varName`, `origName` and `code`.

    -   What are the names of the new indicator variables? Of the continuous variable?

-   Create a vector `newvars` that contains the variable `varName` where `code` is either `clean` or `lev`. Print it.

-   Use `prepare()` to create a new data frame `dframe.treat` that is a one-hot-encoded version of `dframe` (without the outcome column).

    -   Print it and compare to `dframe`.

```{r}
dframe_path <- file.path("..", "00_Datasets", "dframe_vtreat.txt")
dframe <- read.delim(dframe_path)
```

```{r}
# Print dframe
dframe

# Create and print a vector of variable names
(vars <- c("color", "size"))

# Load the package vtreat
library(vtreat)

# Create the treatment plan
treatplan <- designTreatmentsZ(dframe, vars, verbose =FALSE)

# Examine the scoreFrame
(scoreFrame <- treatplan %>%
    use_series(scoreFrame) %>%
    select(varName, origName, code))

# We only want the rows with codes "clean" or "lev"
(newvars <- scoreFrame %>%
    filter(code %in% c("clean", "lev")) %>%
    use_series(varName))

# Create the treated training data
(dframe.treat <- prepare(treatplan, dframe, varRestriction = newvars))
```

Great work! You have successfully one-hot-encoded categorical data. The new indicator variables have `'_lev_'` in their names, and the new cleaned continuous variables have `'_clean'` in their names. The treated data is all numerical, with no missing values, and is suitable for use with xgboost and other R modeling functions.

### **`06-Novel levels`**

When a level of a categorical variable is rare, sometimes it will fail to show up in training data. If that rare level then appears in future data, downstream models may not know what to do with it. When such *novel levels* appear, using `model.matrix` or `caret::dummyVars` to one-hot-encode will not work correctly.

`vtreat` is a "safer" alternative to `model.matrix` for one-hot-encoding, because it can manage novel levels safely. `vtreat` also manages missing values in the data (both categorical and continuous).

In this exercise, you will see how `vtreat` handles categorical values that did not appear in the training set. The treatment plan `treatplan` and the set of variables `newvars` from the previous exercise are still available. `dframe` and a new data frame `testframe` have been pre-loaded.

-   Print `dframe` and `testframe`.

    -   Are there colors in `testframe` that didn't appear in `dframe`?

-   Call [**`prepare()`**](https://www.rdocumentation.org/packages/vtreat/topics/prepare) to create a one-hot-encoded version of `testframe` (without the outcome). Call it `testframe.treat` and print it.

    -   Use the `varRestriction` argument to restrict to only the variables in `newvars`.

    -   How are the yellow rows encoded?

```{r}
testframe_path <- file.path("..", "00_Datasets", "testframe_vtreat.txt")
testframe <- read.delim(testframe_path)
```

```{r}
# treatplan is available
summary(treatplan)

# newvars is available
newvars

# Print dframe and testframe
dframe
testframe

# Use prepare() to one-hot-encode testframe
(testframe.treat <- prepare(treatplan, testframe, varRestriction = newvars))
```

Good work! As you saw, vtreat encodes novel colors like yellow that were not present in the data as all zeros: 'none of the known colors'. This allows downstream models to accept these novel values without crashing.

### **`07-vtreat the bike rental data`**

In this exercise, you will create one-hot-encoded data frames of the July/August bike data, for use with `xgboost` later on.

The data frames `bikesJuly` and `bikesAugust` have been pre-loaded.

For your convenience, we have defined the variable `vars` with the list of variable columns for the model.

-   Load the package `vtreat`.

-   Use [**`designTreatmentsZ()`**](https://www.rdocumentation.org/packages/vtreat/topics/designTreatmentsZ) to create a treatment plan `treatplan` for the variables in `vars` from `bikesJuly` (the training data).

    -   Set the flag `verbose=FALSE` to prevent the function from printing too many messages.

-   Fill in the blanks to create a vector `newvars` that contains only the names of the `clean` and `lev` transformed variables. Print it.

-   Use [**`prepare()`**](https://www.rdocumentation.org/packages/vtreat/topics/prepare) to create a one-hot-encoded training data frame `bikesJuly.treat`.

    -   Use the `varRestrictions` argument to restrict the variables you will use to `newvars`.

-   Use `prepare()` to create a one-hot-encoded test frame `bikesAugust.treat` from `bikesAugust` in the same way.

-   Call `str()` on both prepared test frames to see the structure.

```{r}
# The outcome column
(outcome <- "cnt")

# The input columns
(vars <- c("hr", "holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed"))

# Load the package vtreat
library(vtreat)

# Create the treatment plan from bikesJuly (the training data)
treatplan <- designTreatmentsZ(bikesJuly, vars, verbose = FALSE)

# Get the "clean" and "lev" variables from the scoreFrame
(newvars <- treatplan %>%
  use_series(scoreFrame) %>%        
  filter(code %in% c("clean", "lev")) %>%  # get the rows you care about
  use_series(varName))           # get the varName column

# Prepare the training data
bikesJuly.treat <- prepare(treatplan, bikesJuly,  varRestriction = newvars)

# Prepare the test data
bikesAugust.treat <- prepare(treatplan, bikesAugust,  varRestriction = newvars)

# Call str() on the treated data
str(bikesJuly.treat)
str(bikesAugust.treat)

```

\


\





\

\
